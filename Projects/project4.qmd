---
title: "Project 4 - [Can you predict that?]"
subtitle: "Course DS 250"
author: "[Ana Soto]"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#Load libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from xgboost import XGBClassifier  

```


```{python}
#Load data

data = pd.read_csv("https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv")

optional_data = pd.read_csv("https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv")
```

## Elevator pitch

_Throughout this project, I gained a crucial insight into the importance of feature engineering. Regardless of the model or tuning parameters I tried, the preparation of data and the creation of additional features play crucial roles in improving prediction accuracy.The most important feature created, was the "overall condition sum" feature. When looking at the data, I thought it would be beneficial to join all of these condition columns into one. It turns out it was a really useful feature, according to the feature importance. Thus, reiterating the importance of preparing the data and feature engineering._


## QUESTION 1

__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__

```{python}

fig = px.scatter(data, x='netprice', y='before1980', title='Garage size vs. Before1980', 
                 labels={'netprice': 'Net price', 'before1980': 'Before 1980'})
fig.show()

fig = px.scatter(data, x='totunits', y='before1980', title='Dwelling Units vs. Before1980', 
                 labels={'totunits': 'Dwelling Units', 'before1980': 'Before 1980'})
fig.show()
  

fig = px.scatter(data, x='stories', color='before1980', 
                 title='Stories vs. Before1980', 
                 labels={'stories': 'Stories', 'before1980': 'Before 1980'})
fig.show()

```

_After looking at the data, I found three columns interesting: "Net Price (netprice)", "dwelling units (totunits)", and "stories." First, the "netprice" column is great for predicting whether a property was built before or after 1980. As there is a clear division in the comparison visualization. Given to the inflation in house prices over the years. Second, with "dwelling units," buildings with more than five units are usually older, built before 1980. Last, in terms of number of "stories", most older houses had one or two floors, sometimes three, but very few had four floors._

## QUESTION 2

__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.__

```{python}

data_merged = pd.merge(data, optional_data, on='parcel', how='left')

#missing values
imputer = SimpleImputer(strategy='mean')
data_merged[['totunits', 'stories', 'numbaths', 'sprice', 'deduct', 'netprice', 'tasp']] = imputer.fit_transform(data_merged[['totunits', 'stories', 'numbaths', 'sprice', 'deduct', 'netprice', 'tasp']])

#feature columns
feature_columns = ['netprice', 'nocars', 'totunits', 'stories', 'numbdrm', 'numbaths', 'sprice', 'deduct', 'condition_Good', 'syear']

# neighborhood features to the feature columns list
neighborhood_features = ['nbhd_1', 'nbhd_2', 'nbhd_3', 'nbhd_4', 'nbhd_5', 'nbhd_101', 'nbhd_104', 'nbhd_105', 'nbhd_106']
feature_columns.extend(neighborhood_features)

# overall condition and quality features

overall_condition = ['condition_Excel', 'condition_Fair', 'condition_Good', 'condition_VGood']
overall_quality = ['quality_A', 'quality_B', 'quality_C', 'quality_D', 'quality_X']

# sum them up
data_merged['overall_condition_sum'] = data_merged[overall_condition].sum(axis=1)

# add overall condition and quality to feature list
feature_columns.append('overall_condition_sum')

# set target and features
X = data_merged[feature_columns]
y = data_merged['before1980']

#split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#model
model = XGBClassifier(
    max_depth=6,
    learning_rate=0.2, 
    n_estimators=100, 
    gamma=0.1,       
    random_state=42
)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

```

_For my classification model, I decided to use XGBoost as its a reliable but not super complex model. As far as the parameters I used, a max depth of 6. If I went any higher, it seemed like it got a bit complicated and my accuracy score went down.  I just played around with the learning rate, nestimators to see if it would help the accuracy._


## QUESTION 3

__Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.__

```{python}

#feature importances
feature_importances = model.feature_importances_

#feature names
feature_names = X.columns

sorted_indices = feature_importances.argsort()[::-1]
sorted_importances = feature_importances[sorted_indices]
sorted_features = feature_names[sorted_indices]

#plot feature importances
plt.figure(figsize=(10, 6))
plt.bar(range(len(sorted_importances)), sorted_importances, tick_label=sorted_features)
plt.xticks(rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.show()

```

_I decided to include neighborhood data in my analysis as it can be a valuable indicator for the model. I began by selecting relevant columns like net price, stories, and dwelling units, which I had identified as important in earlier analysis. Then, I explored feature engineering, creating new metrics such as overall rating and quality. One standout feature was "overall_condition_sum," which combines various property conditions. Additionally, features like number of stories, specific neighborhoods, and property prices added useful information to the analysis.

overall_condition_sum : sum combined conditions

stories: number of stories

neighborhoods: specific neighborhoods

nocars: number of cars in garage

numbdrm: number of bedrooms

netprice: net price of home

sprice: selling price

deduct: deduction from selling price

syear: year that home was sold
_

## QUESTION 4

__Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.__

```{python}

#accuracy, recall, f1 score
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Recall score:", recall)
print("F1 score:", f1)

```

_In assessing the model's performance, I opted for three metrics: accuracy, recall, and F1 score. The goal was to achieve an accuracy score exceeding 90%. Accuracy measures how many predictions are correct out of all predictions made. Recall evaluates the model's capability to predict true positives accurately. The F1 score, being an average of precision and recall, is useful for datasets where classes are unevenly distributed. It's beneficial as it considers both false positives and false negatives in its calculation._
